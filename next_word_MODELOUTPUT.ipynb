{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**MODEL OUTPUT**"
      ],
      "metadata": {
        "id": "HCZjOmI2ulsG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just run this script it will download all dependencies ðŸ‘‡"
      ],
      "metadata": {
        "id": "QWB5V_qz4eag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown \"https://drive.google.com/uc?id=1J78MUhgrvq720oQkCEkIlxo9becbG0BY\"\n",
        "!gdown \"https://drive.google.com/uc?id=13TeQjXINKQvFR_1sj6OT2j63OtwJXwzn\"\n",
        "!gdown \"https://drive.google.com/uc?id=1tebdUijCsq8z7r5bo3_Hoq5jAM0fotzR\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEMgnCIM3sD3",
        "outputId": "fdf5cdb4-19c4-4128-e156-6b8cebabd961"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1J78MUhgrvq720oQkCEkIlxo9becbG0BY\n",
            "To: /content/vocab.pkl\n",
            "100% 38.2k/38.2k [00:00<00:00, 21.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=13TeQjXINKQvFR_1sj6OT2j63OtwJXwzn\n",
            "To: /content/trained_model.pkl\n",
            "100% 856k/856k [00:00<00:00, 82.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1tebdUijCsq8z7r5bo3_Hoq5jAM0fotzR\n",
            "To: /content/inv_vocab.pkl\n",
            "100% 38.3k/38.3k [00:00<00:00, 49.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch                        # use to do tensor computation fast\n",
        "import torch.nn as nn               # nn pytorch dedicated library for neural networks\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "74FdoFCFgqiO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class TinyNextWordModel(nn.Module):\n",
        "    def __init__(self, vocab_size=2000, embed_dim=32, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)\n",
        "        out, _ = self.gru(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ],
      "metadata": {
        "id": "YifkbNJBgpiy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def simple_tokenizer(text):\n",
        "    return text.lower().split()\n"
      ],
      "metadata": {
        "id": "0S-ytMFsjv5S"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = torch.load(\"vocab.pkl\")\n",
        "inv_vocab = torch.load(\"inv_vocab.pkl\")\n",
        "vocab_size = min(len(vocab), 2000)\n",
        "model99 = torch.load(\"trained_model.pkl\", weights_only=False)"
      ],
      "metadata": {
        "id": "Jg8pt8a8mh5P"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "this is the main code ðŸ‘‡"
      ],
      "metadata": {
        "id": "C5qcYF-65Fgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, text_input, seq_len=20, top_k=5):\n",
        "    model.eval()\n",
        "    tokens = simple_tokenizer(text_input)  # same tokenizer as training\n",
        "    encoded = [vocab.get(t, 0) for t in tokens[-seq_len:]]\n",
        "    if len(encoded) < seq_len:\n",
        "        encoded = [0]*(seq_len - len(encoded)) + encoded\n",
        "    x = torch.tensor(encoded).unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model(x)\n",
        "        probs = F.softmax(out, dim=1)\n",
        "        top_probs, top_idxs = probs.topk(top_k, dim=1)\n",
        "\n",
        "    results = [(inv_vocab.get(idx.item(), \"unknown\"), prob.item())\n",
        "               for idx, prob in zip(top_idxs[0], top_probs[0])]\n",
        "    return results"
      ],
      "metadata": {
        "id": "p1kMVf6VjhHT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "this is the driving code that call predict function ðŸ‘‡"
      ],
      "metadata": {
        "id": "4_USlthI5Jt_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "as these is biased towards space theme i reccomendes to use question like:\n",
        "\n"
      ],
      "metadata": {
        "id": "-bWxwTC3TwTo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "solar ?\n"
      ],
      "metadata": {
        "id": "uuMdp07EUZp9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "pluto is a ?\n"
      ],
      "metadata": {
        "id": "JRNqX1-cUbdb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "black ?\n"
      ],
      "metadata": {
        "id": "9WQp2Q5XUdrH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "( ofc dont use question mark)"
      ],
      "metadata": {
        "id": "AJ8duMCSUjZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ques = \" sun is very \"\n",
        "preds = predict(model99, ques, top_k=5)\n",
        "for word, prob in preds:\n",
        "    print(word,\"             \", f\"{prob*100:.2f}\",\"%  probability\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ne9Ti09xjhAX",
        "outputId": "47dca4a8-7c7f-47af-cb4d-f49c74e82faf"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<unk>               62.54 %  probability\n",
            "working               12.36 %  probability\n",
            "large               9.46 %  probability\n",
            "place               3.13 %  probability\n",
            "visible               2.10 %  probability\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QCmK5iYJiBeC"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}